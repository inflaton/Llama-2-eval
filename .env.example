# LLM_MODEL_TYPE=openai
LLM_MODEL_TYPE=huggingface
# LLM_MODEL_TYPE=mosaicml

# LANGCHAIN_DEBUG=true

HF_RP=1.095
ML_RP=1.05

OPENAI_API_KEY=

# OPENAI_MODEL_NAME=gpt-4
OPENAI_MODEL_NAME=gpt-3.5-turbo

# cpu, mps or cuda:0 - if unset, use whatever detected
HF_PIPELINE_DEVICE_TYPE=

HUGGINGFACE_AUTH_TOKEN=

DISABLE_MODEL_PRELOADING=true

# uncomment one of the below to load corresponding quantized model
# LOAD_QUANTIZED_MODEL=4bit
# LOAD_QUANTIZED_MODEL=8bit

# USING_TORCH_BFLOAT16=true

# HUGGINGFACE_MODEL_NAME_OR_PATH="meta-llama/Llama-2-70b-chat-hf"
# HUGGINGFACE_MODEL_NAME_OR_PATH="meta-llama/Llama-2-13b-chat-hf"
HUGGINGFACE_MODEL_NAME_OR_PATH="meta-llama/Llama-2-7b-chat-hf"

# HUGGINGFACE_MODEL_NAME_OR_PATH="lmsys/vicuna-13b-v1.1"
# HUGGINGFACE_MODEL_NAME_OR_PATH="lmsys/vicuna-7b-v1.1"
# HUGGINGFACE_MODEL_NAME_OR_PATH="lmsys/fastchat-t5-3b-v1.0"

# HUGGINGFACE_MODEL_NAME_OR_PATH="TheBloke/wizardLM-7B-HF"
# HUGGINGFACE_MODEL_NAME_OR_PATH="nomic-ai/gpt4all-j"

MOSAICML_MODEL_NAME_OR_PATH="mosaicml/mpt-7b-instruct"
